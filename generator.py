import torch
import torch.nn.functional as F
import sys
import numpy as np
from typing import List
from architecture import HyNet
from datasets import BrownDataset
import transform
import tlib
from torch.cuda.amp import autocast, GradScaler


class Generator:
    def __init__(self):
        pass

    @staticmethod
    def adv_gen(x: List[torch.Tensor], adv_model,
                iters: int, step: float, device, padding_mode="reflection",
                output_rec=False, output_confidence=False, half=False, opts=None):
        """
        generate adversarial samples using STN bilinear interpolation module
        INPUT:
        x: clean samples
        adv_model: model to be attacked
        gen_model: model to generate synthesis samples
        iters: number of step for generating adv samples
        device: device of "x"
        step: step length
        padding_mode:
        output_rec:
        opts:
        OUTPUT:
        adv samples generated by anchor & positive samples. If output_rec is set to True, return reconstructed smaples additionally
        """

        adv_model.eval()
        adv_model.zero_grad()

        # samples
        x_a_rec = None
        x_p_rec = None
        # label a.k.a. confidence score
        l_a_adv = None
        l_p_adv = None

        # theta_a_ori & theta_p_ori are leaf params
        theta = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)
        theta_a_ori = theta.expand(x[0].size()[0], 1, 6).to(device)
        theta_p_ori = theta.expand(x[0].size()[0], 1, 6).to(device)
        theta_a_ori.requires_grad_(True)
        theta_p_ori.requires_grad_(True)
        theta_a = theta_a_ori.view(-1, 2, 3).to(device)
        theta_p = theta_p_ori.view(-1, 2, 3).to(device)

        x_a = x[0].to(device)
        x_p = x[1].to(device)

        for i in range(iters):
            if half:
                with autocast():
                    grid_a = F.affine_grid(theta_a, x[0].size(), align_corners=True).to(device)
                    grid_p = F.affine_grid(theta_p, x[0].size(), align_corners=True).to(device)
                    x_adv_a = F.grid_sample(x_a, grid_a.float(), padding_mode=padding_mode, align_corners=True).to(
                        device)
                    x_adv_p = F.grid_sample(x_p, grid_p.float(), padding_mode=padding_mode, align_corners=True).to(
                        device)
                    if output_rec:
                        if x_a_rec is None:
                            x_a_rec = x_adv_a.clone().detach()
                        if x_p_rec is None:
                            x_p_rec = x_adv_p.clone().detach()
                    desc_a = adv_model(x_adv_a)
                    desc_p = adv_model(x_adv_p)
                    # gradient direction as gradient ascent direction
                    with torch.no_grad():
                        desc_a_rec = F.normalize(adv_model(x_a_rec), p=2, dim=1)
                        desc_p_rec = F.normalize(adv_model(x_p_rec), p=2, dim=1)
                    loss = torch.mean(tlib.compute_distance_unit_l2(torch.cat([desc_a, desc_p], dim=0),
                                                                    torch.cat([desc_p_rec, desc_a_rec], dim=0)))
                    loss.backward()

                    with torch.no_grad():
                        grad_sign_a_ori = theta_a_ori.grad.data.detach().sign()
                        grad_sign_p_ori = theta_p_ori.grad.data.detach().sign()
                        grad_sign_norm_a_ori = grad_sign_a_ori / grad_sign_a_ori.norm(dim=[1, 2], keepdim=True)
                        grad_sign_norm_p_ori = grad_sign_p_ori / grad_sign_p_ori.norm(dim=[1, 2], keepdim=True)
                        # Gradient ASCENT
                        theta_a_ori = theta_a_ori + grad_sign_norm_a_ori * step
                        theta_p_ori = theta_p_ori + grad_sign_norm_p_ori * step

                    theta_a_ori.requires_grad_(True)
                    theta_p_ori.requires_grad_(True)
                    adv_model.zero_grad()

                    theta_a = theta_a_ori.view(-1, 2, 3).to(device)
                    theta_p = theta_p_ori.view(-1, 2, 3).to(device)

                    if output_confidence is True:
                        l_a_adv = tlib.calc_iou(theta_a, x[0].size()[2])
                        l_p_adv = tlib.calc_iou(theta_p, x[0].size()[2])
            else:
                grid_a = F.affine_grid(theta_a, x[0].size(), align_corners=True).to(device)
                grid_p = F.affine_grid(theta_p, x[0].size(), align_corners=True).to(device)
                x_adv_a = F.grid_sample(x_a, grid_a.float(), padding_mode=padding_mode, align_corners=True).to(device)
                x_adv_p = F.grid_sample(x_p, grid_p.float(), padding_mode=padding_mode, align_corners=True).to(device)
                if output_rec:
                    if x_a_rec is None:
                        x_a_rec = x_adv_a.clone().detach()
                    if x_p_rec is None:
                        x_p_rec = x_adv_p.clone().detach()
                desc_a = adv_model(x_adv_a)
                desc_p = adv_model(x_adv_p)
                # gradient direction as gradient ascent direction
                with torch.no_grad():
                    desc_a_rec = F.normalize(adv_model(x_a_rec), p=2, dim=1)
                    desc_p_rec = F.normalize(adv_model(x_p_rec), p=2, dim=1)
                loss = torch.mean(tlib.compute_distance_unit_l2(torch.cat([desc_a, desc_p], dim=0),
                                                                torch.cat([desc_p_rec, desc_a_rec], dim=0)))
                loss.backward()

                with torch.no_grad():
                    grad_sign_a_ori = theta_a_ori.grad.data.detach().sign()
                    grad_sign_p_ori = theta_p_ori.grad.data.detach().sign()
                    grad_sign_norm_a_ori = grad_sign_a_ori / grad_sign_a_ori.norm(dim=[1, 2], keepdim=True)
                    grad_sign_norm_p_ori = grad_sign_p_ori / grad_sign_p_ori.norm(dim=[1, 2], keepdim=True)
                    # Gradient ASCENT
                    theta_a_ori = theta_a_ori + grad_sign_norm_a_ori * step
                    theta_p_ori = theta_p_ori + grad_sign_norm_p_ori * step

                theta_a_ori.requires_grad_(True)
                theta_p_ori.requires_grad_(True)
                adv_model.zero_grad()

                theta_a = theta_a_ori.view(-1, 2, 3).to(device)
                theta_p = theta_p_ori.view(-1, 2, 3).to(device)

                if output_confidence is True:
                    l_a_adv = tlib.calc_iou(theta_a, x[0].size()[2])
                    l_p_adv = tlib.calc_iou(theta_p, x[0].size()[2])

        grid_a = F.affine_grid(theta_a, x[0].size(), align_corners=True)
        grid_p = F.affine_grid(theta_p, x[0].size(), align_corners=True)
        x_a_adv = F.grid_sample(x_a, grid_a.float(), padding_mode=padding_mode, align_corners=True).to(device)
        x_p_adv = F.grid_sample(x_p, grid_p.float(), padding_mode=padding_mode, align_corners=True).to(device)

        adv_model.zero_grad()
        adv_model.train()

        # IMPORTANT: detach adv samples from current computational graph
        out_dict = {"x_a_rec": x_a_rec.clone().detach() if x_a_rec is not None else None,
                    "x_p_rec": x_p_rec.clone().detach() if x_p_rec is not None else None,
                    "x_a_adv": x_a_adv.clone().detach() if x_a_adv is not None else None,
                    "x_p_adv": x_p_adv.clone().detach() if x_p_adv is not None else None,
                    "l_a_adv": l_a_adv.clone().detach() if l_a_adv is not None else None,
                    "l_p_adv": l_p_adv.clone().detach() if l_p_adv is not None else None}
        return out_dict

    @staticmethod
    def rand_gen(x: List[torch.Tensor], iters, device, step, distribution="uniform",
                 padding_mode="reflection", output_rec=False, output_confidence=False):
        """
        generate random samples using STN bilinear interpolation module
        INPUT:
        x: clean samples
        iters: number of step for generating adv samples, samples are subject to uniform distribution U(-step * iters, step * iters)
        device: device of "x"
        step: step length
        debug: whether debug mode is on. If on, save generated samples automatically
        OUTPUT:
        random samples generated by anchor & positive samples
        """

        # samples
        x_a_rec = None
        x_p_rec = None
        # label a.k.a. confidence score
        l_a_adv = None
        l_p_adv = None

        theta = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)
        theta = theta.view(-1, 2, 3)
        theta_a = theta.expand(x[0].size()[0], 2, 3).to(device)
        theta_p = theta.expand(x[0].size()[0], 2, 3).to(device)
        x_a = x[0].to(device)
        x_p = x[1].to(device)

        if output_rec:
            grid_a = F.affine_grid(theta_a, x[0].size(), align_corners=True)
            grid_p = F.affine_grid(theta_p, x[0].size(), align_corners=True)
            x_a_rec = F.grid_sample(x_a, grid_a.float(), padding_mode=padding_mode, align_corners=True).to(device).clone().detach()
            x_p_rec = F.grid_sample(x_p, grid_p.float(), padding_mode=padding_mode, align_corners=True).to(device).clone().detach()

        if distribution == "uniform":
            for i in range(iters):
                delta_norm_a = torch.rand(x[0].size()[0], 2, 3) * 2 - 1
                delta_norm_p = torch.rand(x[0].size()[0], 2, 3) * 2 - 1
                delta_norm_a = delta_norm_a / delta_norm_a.norm(dim=[1, 2], keepdim=True)
                delta_norm_p = delta_norm_p / delta_norm_p.norm(dim=[1, 2], keepdim=True)
                theta_a = theta_a + delta_norm_a.to(device) * step
                theta_p = theta_p + delta_norm_p.to(device) * step
        elif distribution == "global_uniform":
            delta_norm_a = torch.rand(x[0].size()[0], 2, 3) * 2 - 1
            delta_norm_p = torch.rand(x[0].size()[0], 2, 3) * 2 - 1
            delta_norm_a = delta_norm_a / delta_norm_a.norm(dim=[1, 2], keepdim=True)
            delta_norm_p = delta_norm_p / delta_norm_p.norm(dim=[1, 2], keepdim=True)
            theta_a = theta_a + delta_norm_a.to(device) * step * iters
            theta_p = theta_p + delta_norm_p.to(device) * step * iters
        else:
            raise NotImplementedError

        with torch.no_grad():
            grid_a = F.affine_grid(theta_a, x[0].size(), align_corners=True)
            grid_p = F.affine_grid(theta_p, x[0].size(), align_corners=True)
            x_a_adv = F.grid_sample(x_a, grid_a.float(), padding_mode=padding_mode, align_corners=True).to(device)
            x_p_adv = F.grid_sample(x_p, grid_p.float(), padding_mode=padding_mode, align_corners=True).to(device)

        if output_confidence is True:
            l_a_adv = tlib.calc_iou(theta_a, x[0].size()[2])
            l_p_adv = tlib.calc_iou(theta_p, x[0].size()[2])

        out_dict = {"x_a_rec": x_a_rec.clone().detach() if x_a_rec is not None else None,
                    "x_p_rec": x_p_rec.clone().detach() if x_p_rec is not None else None,
                    "x_a_adv": x_a_adv.clone().detach() if x_a_adv is not None else None,
                    "x_p_adv": x_p_adv.clone().detach() if x_p_adv is not None else None,
                    "l_a_adv": l_a_adv.clone().detach() if l_a_adv is not None else None,
                    "l_p_adv": l_p_adv.clone().detach() if l_p_adv is not None else None}
        return out_dict


if __name__ == "__main__":
    device = "cuda"
    advg = Generator()
    # x = [torch.rand([1, 1, 32, 32]), torch.rand([1, 1, 32, 32])]
    net = HyNet().to(device)
    net.load_state_dict(torch.load("./pretrained/HyNet_LIB.pth", map_location=device))

    tforms = transform.get_input_transform_no_norm(32)
    dset = BrownDataset(root="./data/brown", name="liberty", download=True,
                        train=True, transform=tforms, triplet=False, data_aug=False, output_index=False)
    train_loader = torch.utils.data.DataLoader(dset, batch_size=64, shuffle=False,
                                               num_workers=0 if sys.platform == "win32" else 8,
                                               drop_last=True, pin_memory=True,
                                               worker_init_fn=lambda x: np.random.seed(np.random.get_state()[1][0] + x))
    for batch_idx, batch_data in enumerate(train_loader):
        data = torch.cat(batch_data, dim=0).to(device)
        data_adv = advg.adv_gen(x=batch_data, adv_model=net, iters=10, step=5, device=device, padding_mode="reflection")
